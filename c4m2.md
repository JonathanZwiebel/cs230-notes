# Deep Convolutional Models

# Why Look at Case Studies?
CV research has been figuring out how to put together CONV, POOL, and FC layers

Latest network architecture
LeNet-5
AlexNet
VGG
ResNet (152 layer deep NN)
Inception NN

# Classic Networks
## LeNet-5
We used to recognize handwritten 32 x 32 x 1 images
Use CONV 6 5x5 filters with stride of 1 and no padding
Use POOL with 2x2 filters and stride of 2
Use CONV 16 5x5 filters with stride of 1 and no padding
Use POOL with 2x2 filters and stride of 2
This now gets us a 5x5x16, flatten
Use FC layer to 120
Use FC layer to 84
Use FC layer to softmax / logistic regression
Has 60k parameters

As you go across layers, channels increase and size decreases
CONV ReLU POOL CONV ReLU POOL ... CONV ReLU POOL FC FC ... FC OUTPUT

People used sigmoid / tanh layers

## AlexNet
227x227x3
Use CONV 96 channels on 11x11 filters with stride of 4
55x55x96
Use POOL with 3x3 filters and stride of 2
27x27x96
Use CONV 256 channels on 5x5 filters with stride of 1 and same padding
27x27x256
Use POOL with 3x3 filters and stride of 2
13x13x256
Use CONV 384 channels and 3x3 filters with stride of 1 and same padding
13x13x384
Use CONV 384 channels and 3x3 filters with stride of 1 and same padding
13x13x384
Use CONV 256 channels and 3x3 fitlers with stride of 1 and same padding
13x13x256
Use POOL with 3x3 filters and striede of 2
6x6x256
Flatten into 9216
FC into 4096
FC into 4096
Softmax into 1000

Local response normalization is normalizing values in a single pixel across all of the channels

## VGG-16
All CONV layers are 3x3, s=1, same
All POOL layers are 2x2, s=2

224x224x3
CONV, 64
224x224x64
CONV, 64
224x224x64
POOL
112x112x64
CONV, 128 
112x112x128
CONV, 128 
112x112x128
POOL
56x56x128
CONV, 256
CONV, 256
CONV, 256
POOL
CONV 512 x3
POOL
CONV 512 x3
POOL
7x7x512
FC 4096
FC 4096
Softmax

# ResNets
Residual networks are difficult to train

Use residual blocks
a^[l] --linear--> --ReLU--> a^[l+1] --linear--> --ReLU--> a^[l+2]
For info to get from a^[l] to a^[l+n] you need a bunch

Just add a^[l] after linearization (before ReLU)
This is a skipped or shortcut layer

Many residual blocks together forms a ResNet
Addding a shortcut every 2 layers is a residual network

# Why ResNets Work
Networks that are too deep are harder to train

ex:
X --> Big NN --> a^[l]
x --> Big NN --> a^[l] --(layer)--> a^[l+1] --(layer)--> a^[l+2]
There is a connection from a^[l] to the layer after a^[l+1]s 
There's are all ReLU

a^[l+2] = g(z^[l+2] + a^[l]) = g(W^[l+2]a^[l+1] + b^[l+2] + a^[l])
If the weight decayed to below 0 then the terms go away and g(a^[l]) = a^[l]

The ID function is easy for the residual block to learn
Adding extra layers doesn't hurt the network too much, easy to learn the ID function

Without resnet it is hard to learn the identity function
If we do the addition then dimensions must be the same, this means we need SAME convolutions

If the dimensions change then you need a matrix that lienarly goes from older size to newer size

ResNets have a lot of 3x3 SAME convolutions and a bunch of jumps
You make an adjustment to dimensions with either a static matrix or a paramters

# Networks in Networks and 1x1 Convolutions
A 1x1 convolution seems to just scale stuff
ex:
You have a 6x6x32 image and a 1x1x32 filter
Looks at each of the 36 pixels and for each find the ReLu of the linear average sum

It is a fully connected NN that goes from the 6x6x32 image to a 6x6 grid

Called network in network

ex:
28x28x192 volume and you want to reduce number of channels
28x28x32 convolution, this will find the importance of each filter and find some average sum of values

# Inception Network Motivation
ex:
28x28x192 image

An inception layer has multiple different kinds of layers
1x1 same convolution would give you 28x28x64 volume
3x3 same convolution would give you a 28x28x128 volume
5x5 same convolution would give you a 32x32x32  volume
POOL outputs a 28x28x32 volume (would need to be with padding and s=1)

Input of 28x28x192
Output of 28x28x256 (with a range of different things)

Inception layers are computationally complex to calculate

Use 1x1 convolutions to reduce the number of channels first
Then use a 5x5 convolution to get the desired size
This is called bottlenecking

28x28x192
1x1 convolution: 2.4M computations
28x28x16
5x5 convolution: 10.0M computations
28x28x32

# Inception Network
Each connection in the inception is a combination of different layers in series
1x1
1x1 into 3x3
1x1 into 5x5
pool into 1x1

All should be SAME convolutions
This is one inception modue
You just put a bunch of them together

True inception network has the string of flatten, FC, FC, softmax at intermediate values
You then backtrain through these values

# Using Open Source Implementation
Use open-source work
Use GitHub

# Transfer Learning
Take pre-trained weights
Download codes and weights
ImageNet is a good pre-trained network

Get rid of softmax layer and other and replace stuff with your own
Freeze parameters in first n layers of the network
You speed up by computing activations up through the frozen layers
Alternatively use the existing weights just as initialization

# Data Augmentation
Mirroring
Cropping
Rotating
Distortions
Color-Shifting (constant shifts to RGB)

Implementation:
One thread for distorting and one thread for training

# State of Computer Vision
