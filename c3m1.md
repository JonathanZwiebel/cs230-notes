# ML Strategy

# Why ML Strategy
ML strategy is how to select ideas that improve your model
Tons and tons of ideas that could improve the model

# Orthogonalization
There are tons of things to tune
Orthogonalization is understanding what effect changing each hyperparameter / feature has on the performance of the model
Knobs are designed so that each knob does only a single thing

1. Fit training set well on cost function (Bias)
    Bigger network, Different training
2. Fit dev set well on cost function (Variance)
    Regularization, More data
3. Fit test set well on cost function
    Bigger dev set
4. Performs well in real world
    Cost function

Early stopping is a very difficult knob to use
    increases bias while decreasing variance

# Single Number Evaluation Metric
Setting up a single real number evaluation metric

Precision - Of ones recognized, what % are true
Recall - Of true exmaples, what % are recognized
Precision and recall are tradeoff
The problem with P/R is that it gives you two numbers

Use the F1 score (harmonic average of precision and recall)
Requires a well-defined dev set
Allows you to iterate more easily

ex: As errors
    us  China   India   Other
A   3   7       5       9
B   5   6       5       10

Given four numbers you have difficulty to compare them
Compute an average or harmonic average

# Satisficing and Optimizing Metric
ex. let's say you have accuracy and running time
It is strange to combine them because their scales are so off
Choose a classifier that maximizes accuracy given some constraings on running time

Run time is satisficing metric as it just needs to be met
Accuracy is an optimizing metric as it should be optimized

With N metrics:
    Choose 1 to optimize
    The rest are satisficing

ex: Wake words
Accuracy
Number of False Positives

ex: maximize accuracy with <= 1 false positive every 24 hours

# Train/Dev/Test Distributions
Dev set often called hold out cross-validation set
Iterate to improve dev set performance

ex: cat classifier working in various regions
Your dev and test sets must come from the same distribution
Training set can come from a different dataset

# Size of Dev and Test Sets
Old rule of tumb was 60/20/20 split
With more data splits like 98/1/1 are more reasonable

The test set should be big enough to have high confidence in overall performance of the system
Should almost have train, dev, and test set

# When to Change Dev/Test Sets and Metrics
ex: using classification error
Alg A has 3% error but let's through bad images
Alg B has 5% error

You prefer a different algorithm than the metric + dev set

Error is 1 / m_dev * sum(I{y_pred != y})
One way to fix this is to add w^{i} which is 1 if image is not bad and 100 if image is bad
Examples that are bad are penalized much more heavily

Define metric first and then optimize for that metric

# Why Human-Level Performance
Accuracy increases rapidly until human-level performance is reached
Bayes optimal error is the best possible error given the data given

Bayes error represents theoretical limit of modelling
1. Human-level performance is close to Bayes optimal
2. Below human-level performance
    You can get labeled data from humans
    Manual error analysis
    Analyze bias and variance

# Avoidable Bias
ex:
Human error    is 1%
Training error is 8%
Dev error      is 10%
Remove bias

ex:
Human error     is 7.5%
Training error  is 8%
Dev error       is 10%
Focus on reducing variance

Human-level error is a proxy for Bayes error

Training error vs Bayes error is avoidable
Dev error vs Training error is variance
Doing better than Bayes erorr is overfitting

# Understanding Human-Level Performance
ex:
Human - 3% error
Docotr - 1% error
Experienced doctor - 0.7% error
Team of experienced doctors - 0.5% error

We consider team of experienced doctors as 0.5% error

ex:
human-level error is 1%
training error is 5%
dev error is 6%

We have 4% of avoidable bias
We have 1% of variance

Non-zero Bayes error means we can only avoid training error - human-level error (or Bayes error)

# Surpasing Human-Level Performance
ex:
Team gets 0.5% error
Human gets 1% error
Training error is 0.6%
Dev error is 0.8%

We have 0.1% avoidable bias
We ahve 0.2% variance

ex:
Team gets 0.5% error
Human gets 1% error
Training error is 0.3% error
Dev error is 0.4%

Hard to tell what Bayes error is 
No way to tell whether you Shouldsubluld focus on bias or variance
Once your surpass human-level performance it is impossible to tell where mistakes are being made

Problems that ML does better than humans
Online advertising 
Product recommendations
Logistics
Loan approvals
(All structured data)

# Improving Your Model Performance
Assumptions:
1. You can fit the training set well (low avoidable bias), bigger network + longer training + change architecture
2. Training set generalizes well to dev/test set (low variance), regularization + more data + change architecture


