# ML Strategy 2

# Carrying Out Error Analysis
If your test error is less than human error than you can optimize
ex: your cat classifier keeps classifying dogs as cats
 - get 100 mislabeled dev set examples
 - count how many of them are dogs
 - ex: 5% of mislabeled dev set exmaples are dogs
 - Best you could do is reduce your test error by 5% by focusing on dogs
 - If 50% of the mislabeled ones were dogs then you could focus on dogs

Create a table where the index are mislabeled images and the values are indications of types of misrecognized images
Helps you prioritize what kind of error to focus on

# Cleaning up Incorrectly Labeled Data
If the data has incorrectly labeled examples:
This is ok to have errors in the training set if they are random

There should be a column for incorrect labels in the dev and test set 
Is it worthwhile to fix the incorrectly labeled examples?

Look at overall dev set error
Look at errors due to incorrect labels
Look at errors due to other causes

Only bother to relabel if the percent of errors that are due to incorrect labels is high
Apply the same process to the dev and test sets at the same time so they come from the same distro
Totally ok if the training set data comes from a different distribution

# Build your First System Quickly Then Iterate
You should build a quick systema as quickly as possible then iterate

# Training and Testing on Different Distributions
It is ok if the training and testing data comes from different distributions
Not ok if the dev and test sets come from different distributions

eX:
You have 200,000 HD images
You care about mobile data for which you have 10,000 images

Shuffle the 200,000 images into your train set
Mix up some of the 10,000 mobile images into the dev and test set and put 5000 into the train se

# Bias and Variance with Mismatched Data Distributions
ex: Bayes error is 0%
Training error is 1%
Dev error is 10%

If the training and dev set come from different distributions then maybe it isn't overfitting, you just have a harder distribution
You also make a training-dev set
 - Should have the same distribution as the training set
 - Carved out of training set
 - used to evaluate over/underfitting and

ex:
Training: 1%
Train-dev: 9%
Dev: 10%
This is variance

Training: 1%
Train-dev: 1.5%
Dev: 10%
Data mismatch problem (the dev and training set distributions are too different)

Training: 10%
Train-dev: 11%
Dev: 12%
This is bias

Human error, Training set error, training-dev set error, dev set error, test set error

It is possible that the training data is much harder than the dev error

Human level error vs training error is avoidable bias
Training error vs train-dev error is variance
Train-dev error vs dev error is data mismatch

# Addressing Data Mismatch
Make the training data ore similar or collect data that is more similar
For example simualte the test set noise over the training set

Artificial data synthesis:
ex: Add noise to clean training audio to make it sound like test set

ex: lets say you have 10,000 hours of clean noise
You have 1 hour of car noise and synthesize with it
Chance that you overfit to the 1 hour of car noise

# Transfer Learning
Train network on one task and use the weights
You can freeze some of the weights in earlier layers or you could just use them to initialize

Transfering task A to task B
Task A and B have the same input X
You have a lot more data for Task A than Task B (you need Task B data)
Low level features for A could be helpful for B

# Multi-Task Learning
ex: car needs to detect pedestrians, cars, stop signs, traffic lights
You have 4 labels and says if they are present in the iamge
You have a 4 vector (not a one-hot vector) that you are trying to learn

The loss for each example is the standard -ylog(y_hat) - (1-y)log(1-y_hat)
The cost function is the average over all training examples of the average loss of each class
multi-task learning works even if stuff isn't fully labelled

Good if you have a lot of similar tasks
Makes sense if you can train a very large network

# What is End-to-end DL?
ex: 
audio (x) --> transcript (y)
A possible implementation is analytic methods for certain parts and ML for different parts
end-to-end is ML from front to back

ex: 
Run software to detect face and run ML over the face

# Whether To Use End-to-end DL
Pros: Lets the data speak, doesn't force human constructs (phonemes), less hand-designing
Cons: Needs lots of end-to-end data, excludes hand-designed components

When to use:
 - Lots of data
 - Difficult intermediate translations
 - 