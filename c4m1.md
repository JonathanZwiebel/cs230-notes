# Convolutional Neural Networks

# Computer Vision
It is important now

Examples:
Image classification. Is X in the image?
Object detection. Where in the image is X? How many and where?
Neural Style Transfer. Repair picture with new style.

Your input feature size gets very large

# Edge Detection Example
Earlier layers detect edges, later layers detect composite obects, final layers ffind steff
ex: you want to detect horizontal and vertical edges

ex: 
You have a 6x6 grayscale image
Construct a 3x3 matrix (a filter) with weights
1 0 -1
1 0 -1
1 0 -1
This is called a kernel

Convolve the 6x6 matrix with the 3x3 Kernel
Pass the filter over the matrix and find the total sum of elements in the matrix by weights in the kernel

In tensflow use
tf.nn.conv2d in Tensorflow
conv2D in Keras

# More Edge Detection
An edge the different way would have different values with a simple 3x3 kernel

the 3x3 vertical edge detection filter we used is just one choice
We did
1 0 -1
1 0 -1
1 0 -1

Sobel filter
1 0 -1
2 0 -2
1 0 -1

Schass filter
3 0 -3
10 0 -10
3 0 -3

Instead of handpicking the numbers try to learn them
Let each of the weights in the kernel be weights

# Padding
6x6 image convolved with 3x3 image gives you a 4x4 image
n x n image convolved with f x f give you n-f+1 x n-f+1
The stuff on the edges is used less and carries less weight
Standard convolution has this issue

Pad the image before convoluting it
ex: Extend a 6x6 image to an 8x8 which convolves down to a 6x6 image
You pad is with a layer of p 0s

Becomes an n + 2p - f + 1 by n + 2p - f + 1

Valid convolution: no padding
n x n * f x f = n - f + 1 x n - f + 1

Same convolution: output is the same as input
n + 2p x n + 2p * f x f = n + 2p - f + 1 x n + 2p - f + 1
p = (f - 1) / 2

# Strided Convolutions
Ex: you want to convolve a 7x7 with a 3x3 with a stride of 2
Instead of stepping over by 1 step you step over by 2 steps

Striding makes stuff much smaller
n x n * f x f with padding p and stride s
(n + 2p - f) / s + 1 x (n + 2p - f) / s + 1

If the fraction isn't an integer then you end up rounding down

Cross-correlations vs Convolution
In math you do a reverse transpose before multiplying 
This flip gives associativity 
(A * B) * C = A * (B * C)

# Convolutions over Volumes
Convoutions over 3D images
6x6x3 images
You need a 3x3x3 convolutional filter

You take the 6x6x3 cube and move the 3x3x3 kernel over it to get a 
6+3-1 x 6+3-1 x 3+3-1 = 4 x 4 x 1 image
 
If you only wanted to detect stuff in a single color then you would set the values to 0 in the other layers
If you don't care about the color then you can set the same values for each color

A different 3x3x3 filter will give a different 4x4x1 image
Combining the two filters now gives you a 4x4x2 box

You have an n x n x n_c cube convolved by f x f x n_c convolution you get a n - f + 1 x n - f + 1 x n_c' where n_c' is the number of filters that you use
The depth of each layer is the number of layers that it has

# One Layer of a Convolutional Network
We still have bias and non-linearlity
Each convolution has a different bias and has the nonlinearity applied to it individually

f^[l] is the size of the filter on layer l
p^[l] is the amount of padding for layer l
s^[l] is the amount of stride for layer l
n_c^[l] is the number of channels (filters) for each layer
Each filter is f^[l] x f^[l] x n_c^[l-1]

The input to the layer is n_H^[l-1] x n_W^[l-1] x n_c^[l-1]
Outputs: n_H^[l] x n_W^[l] x n_c^[l]

n_H^[l] = floor((n_H^[l-1] + 2p^[l] - f^[l]) / s^[l] + 1)

If using BGD then your activation is m x n_H^[l] x n_W^[l] x n_c^[l]
Weights become f^[l] x f^[l] x n_c^[l-1] x n_c^[l]
Bias: 1 x 1 x 1 x n_c^[l] (one for each filter)

# Simple CNN Example
ex: You want to make a ConvNet to determine if there is a cat in an image
39 x 39 x 3 image
n_h^[0] = n_w^[0] = 39
n_c^[o] = 3

f^[1] = 3, s^[1] = 1, p^[1] = 0
10 filters

37 x 37 x 10
f^[2] = 5, s^[2] = 2, p^[2] = 0
20 filters

17 x 17 x 20
f^[3] = 5, s^[3] = 5, p^[3] = 0
40 filters

7 x 7 x 40
Flatten it into an array and feed it into softmax or logistic

All of the work goes down into choosing all of the hyperparameters

Three layers
 - Convolution layer (CONV)
 - Pooling layer (POOL)
 - Fully connected layer (FC)

Most CNN have a mix of conv, pool and fc layers 

# Pooling Layers
Let's say we want to do max pooling on a 4x4 to a 2x2
You break the 4x4 into a bunch of 2x2 grids and then find the max value in each region
This is like a filter size of 2 and a pooling value of 2 (hyperparameters of max pooling)

Max pooling retains the detection of a particular feature

ex:
5x5 input with f=3 and s=1
The output is a 3x3

AVerage pooling:
exactly what you would expect

Hyperparameters for pooling
f = 2, s = 2 typically shrinks
Pooling almost never done with padding

Resizes the same was as a normal convolutional layer
Pooling has no parameters that you learn

# CNN Example
ex: 32 x 32 x 3 image
Trying to do handwriting detection

Most people refer to the CONV and POOL together as a single layer

32 x 32 x 3 (input)
convolution f=5, s=1, 6 filters
28 x 28 x 6 (CONV1)
maxpool f=2, s=2
14 x 14 x 6 (POOL1)
convolution f=5, s=1, 16 filters
10 x 10 x 16 (CONV2)
maxpool f=2, s=2
5 x 5 x 16 (POOL2)
Flatten
400 x 1 vector (FLAT)
fully connected
120 x  1 vector (FC3)
fully connected
84 x 1 vector (FC4)
softmax
10 x 1 vector (OUTPUT)

Height and width decrease while the number of channels increases

# Why Convolutions?
Parameter sharing is why CNNs work
Sharing parameters is easier to train and makes sense as you should use the same feature to train stuff

Sparsity of connections, pixels further away from eacother don't hit eachother
Images shifted and translated gives you a very similar set of values through the image

Cost function is average of loss functions
Use some standard training to reduce J
